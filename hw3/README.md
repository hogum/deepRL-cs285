### Generating Plots
- The plots for each problem can ge re-generated by running the plotting scipt:

`$ python3 generate_plot.py q"$question_number" ` where `$question_number` is the number to the question and should be among `[q1, q2, q3, q4, a5]`.

e.g `python3 generate_plot.py q1   # plots for question 1`



#### Question 1 : Basic Q-learning performance
> performance of the DQN implementation on the game Pong

a) mean 100-episode reward

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q1_average_return.png)

_Mean episode reward in 500k timesteps_

b) the best mean reward

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q1_best_r.png)

_Best mean reward in 500k steps_


---


#### Question 2 : Comparison of DQN to DDQN
> Comparing performance of DQN to the improved DDQN with three average random seeds for each

**DQN**
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_dqn_1 --seed 1
```

```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_dqn_2 --seed 2
```

```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_dqn_3 --seed 3
```

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q2_dqn_all.png)

_Average return for each seed_



**Double DQN**
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_doubledqn_1 --double_q --seed 1
```
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_doubledqn_2 --double_q --seed 2
```
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_doubledqn_3 --double_q --seed 3
```

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q2_ddqn_average_all.png)

_Ddqn average for each seed__

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q2_dqn_ddqn_average.png)

_Dqn and Ddqn returns averaged for the three seeds_



#### Question 3 : Hyperparameters

> A single hyperparameter of choice and run at least three other settings of this hyperparameter, in addition to the one used in Question 1/2, and all four values plotted on the same graph.

> Your choice what you experiment with, but you should explain why you
chose this hyperparameter

> **Hyperameter of Choice**: Learning rate

> We should expect a high learning rate to work well in the beginning when the agent is exploring and not much learning is being done. However, a smaller learning rate will maintain a stable yield in stages where more exploitation is taking place.

- Learning rate annealed from 5e-3 to 5e-4 over 500e3 timesteps

```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q3_hparam1 --optm_spec_name lander_optimizer_annealed_1
```


- Learning rate annealed from 1e-3 to 5e-4 over 500e3 timesteps

```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q3_hparam2 --optm_spec_name lander_optimizer_annealed_2
```


- Learning rate annealed from 1e-4 to 5e-5 over 500e3 timesteps

```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q3_hparam3 --optm_spec_name lander_optimizer_annealed_3
```

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q3.png)

_Average LunarLandar returns for different learning rates. The default learning rate, which is a constant 1e-3 for all 500e3 steps yields the lowest return_

For 500e3 timesteps:
A learning rate of 1e-3 seems to give a high and stable return at the initial stages of training.
Starting with 5e-3, seems to increase the initial stage reward but learning is more unstable.

The default learning rate, 1e-3, however, gives a drastic drop in return value towards the end of training. Compared to this, annealing the 1e-3 lr towards 5e-4 over the training timesteps gives stable and higher rewards.

The lr of 1e-4 (hparam3) was too slow for the number of timesteps done

 
**Actor Critic**

#### Question 4. Sanity check

##### Different target updates for different gradient steps for the critic

> Compare the results for the following settings and report
which worked best. The best setting should match the policy gradient results
from Cartpole in hw2 (200)

```
python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name 1_1 -ntu 1 -ngsptu 1
```

```
python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name 10_10 -ntu 10 -ngsptu 10
```

```
python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name 100_1 -ntu 100 -ngsptu 1
```

```
python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name 1_100 -ntu 1 -ngsptu 100
```

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q4.png)

_`For num_grad_steps_per_target_update * num_target_updates`_, _a more frequent ngsptu yields a higher return value. Updating the target for each gradient step works best, but is more expensive_


#### Question 5.
a) Inverted Pendulum

`export NTU=100 && export NGSTPU=1`

```
python cs285/scripts/run_hw3_actor_critic.py --env_name InvertedPendulum-v2 --ep_len 1000 --discount 0.95 -n 100 -l 2 -s 64 -b 5000 -lr 0.01 --exp_name "$NTU"_"$NGSTPU" -ntu $NTU -ngsptu $NGSTPU
```

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q5_inverted_pendulum.png)

_Evaluation average return_

b) Half-Cheeter

`export NTU=100 && export NGSTPU=1`

```
python cs285/scripts/run_hw3_actor_critic.py --env_name HalfCheetah-v2 --ep_len 150 --discount 0.90 --scalar_log_freq 1 -n 150 -l 2 -s 32 -b 30000 -eb 1500 -lr 0.02 --exp_name "$NTU"_"$NGSTPU" -ntu $NTU -ngsptu $NGSTPU
```

![alt text](https://github.com/mugoh/deepRL-cs285/blob/master/hw3/cs285/.figures/_t_plots/q5_half_cheeter.png)

_Averaged evaluation return for 150 steps_


Hyperparameter
On LunarLandar: anneal learning_rate (Use atari_schedule),
Initial - 1e-3, final-5e-4
Initial - 1e-4, final 5-e-5
default: 1e-3 constant (done in q2)
