#### Question 1 : Basic Q-learning performance
>> performance of my implementation on the game Pong

a) mean 100-episode reward as well as
b) the best mean reward

#### Question 2 : Comparison of DQN to DDQN
>> Three average random seeds.

**DQN**
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_dqn_1 --seed 1
```

```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_dqn_2 --seed 2
```

```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_dqn_3 --seed 3
```


**Double DQN**
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_doubledqn_1 --double_q --seed 1
```
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_doubledqn_2 --double_q --seed 2
```
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q2_doubledqn_3 --double_q --seed 3
```


#### Question 3 : Hyperparameters

>> A single hyperparameter of choice and run at least three other settings of this hyperparameter, in addition to the one used in Question 1/2, and all four values plotted on the same graph.

- Learning rate annealed from 5e-3 to 5e-4 over 500e3 timesteps
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q3_hparam1 --optm_spec_name lander_optimizer_annealed_1
```

- Learning rate annealed from 1e-3 to 5e-4 over 500e3 timesteps
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q3_hparam2 --optm_spec_name lander_optimizer_annealed_2
```

- Learning rate annealed from 1e-4 to 5e-5 over 500e3 timesteps
```
python cs285/scripts/run_hw3_dqn.py --env_name LunarLander-v2 --exp_name q3_hparam3 --optm_spec_name lander_optimizer_annealed_3
```


**Actor Critic**

#### Question 4. Sanity check

##### Different target updates for different gradient steps for the critic

```
python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name 1_1 -ntu 1 -ngsptu 1
```

```
python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name 10_10 -ntu 10 -ngsptu 10
```

```
python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name 100_1 -ntu 100 -ngsptu 1
```

```
python cs285/scripts/run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name 1_100 -ntu 1 -ngsptu 100
```

#### Question 5.
a) Inverted Pendulum

`export NTU=100 && export NGSTPU=1`

```
python cs285/scripts/run_hw3_actor_critic.py --env_name InvertedPendulum-v2 --ep_len 1000 --discount 0.95 -n 100 -l 2 -s 64 -b 5000 -lr 0.01 --exp_name "$NTU"_"$NGSTPU" -ntu $NTU -ngsptu $NGSTPU
```

b) Half-Cheeter

`export NTU=100 && export NGSTPU=1`

```
python cs285/scripts/run_hw3_actor_critic.py --env_name HalfCheetah-v2 --ep_len 150 --discount 0.90 --scalar_log_freq 1 -n 150 -l 2 -s 32 -b 30000 -eb 1500 -lr 0.02 --exp_name "$NTU"_"$NGSTPU" -ntu $NTU -ngsptu $NGSTPU
```


Hyperparameter
On LunarLandar: anneal learning_rate (Use atari_schedule),
Initial - 1e-3, final-5e-4
Initial - 1e-4, final 5-e-5
default: 1e-3 constant (done in q2)
